import streamlit as st
import requests
from bs4 import BeautifulSoup
from datetime import datetime

# Define your news sources
sources = {
    "CSR@å¤©ä¸‹": "https://csr.cw.com.tw/",
    "CSRone": "https://csrone.com/",
    "ETtoday ESG": "https://esg.ettoday.net/",
    "The Guardian Environment": "https://www.theguardian.com/uk/environment",
    "CNN Climate": "https://edition.cnn.com/specials/world/cnn-climate",
    "BBC Environment": "https://www.bbc.com/news/science_and_environment",
    "ä¸­å¤®ç¤¾ å¥åº·ç’°ä¿": "https://www.cna.com.tw/list/ahel.aspx"
}

def fetch_html(url):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return BeautifulSoup(response.text, 'html.parser')
    except Exception as e:
        st.warning(f"âŒ Failed to fetch {url}: {e}")
        return None

def parse_generic(soup, base_url, tag_selector, limit=5):
    articles = []
    seen = set()
    for tag in soup.select(tag_selector):
        title = tag.text.strip()
        link = tag.get('href')
        if not title or not link or title in seen:
            continue
        seen.add(title)
        if link.startswith('/'):
            link = base_url.rstrip('/') + link
        elif not link.startswith('http'):
            continue
        articles.append((title, link))
        if len(articles) >= limit:
            break
    return articles

def get_articles():
    result = {}
    for name, url in sources.items():
        soup = fetch_html(url)
        if not soup:
            result[name] = []
            continue

        if "cw.com.tw" in url:
            articles = parse_generic(soup, url, 'div.article-content a')
        elif "ettoday.net" in url:
            articles = parse_generic(soup, url, 'div.piece h3 a')
        elif "guardian" in url:
            articles = parse_generic(soup, url, 'a.u-faux-block-link__overlay')
        elif "cnn" in url:
            articles = parse_generic(soup, url, 'h3.cd__headline a')
        elif "bbc" in url:
            articles = parse_generic(soup, url, 'a.gs-c-promo-heading')
        elif "cna.com.tw" in url:
            articles = parse_generic(soup, url, 'div.listBox a')
        elif "csrone" in url:
            articles = parse_generic(soup, url, 'h5.card-title a')
        else:
            articles = parse_generic(soup, url, 'a')

        result[name] = articles
    return result

def format_for_notion(articles_by_source):
    date_str = datetime.today().strftime('%Y-%m-%d')
    notion_md = f"## ğŸŒ± ESG Weekly News Digest ({date_str})\n\n"

    for source, articles in articles_by_source.items():
        notion_md += f"### {source}\n"
        if not articles:
            notion_md += "- ç„¡æ³•å–å¾—æ–°èè³‡æ–™\n"
            continue
        for title, link in articles:
            notion_md += f"- [{title}]({link})\n"
        notion_md += "\n"

    return notion_md

# Streamlit Web UI
st.title("ğŸ“¬ ESG æ¯é€±æ–°èå¿«å ±å·¥å…·")

if st.button("ğŸ” æ“·å–æœ¬é€± ESG æ–°è"):
    with st.spinner("æ“·å–ä¸­ï¼Œè«‹ç¨å€™..."):
        news_data = get_articles()
        notion_output = format_for_notion(news_data)
        st.success("æ“·å–å®Œæˆï¼ä»¥ä¸‹ç‚ºå¯è¤‡è£½è²¼ä¸Šçš„ Notion æ ¼å¼ï¼š")
        st.code(notion_output, language='markdown')

st.markdown("---")
st.markdown("æ­¤å·¥å…·ç”± Chris å°ˆç‚º ESG å°ˆæ¥­äººå£«è¨­è¨ˆï¼Œé»ä¸€ä¸‹å°±å¹«ä½ æŠ“å¥½æ¯é€±çš„æ°¸çºŒæ–°èï¼")
